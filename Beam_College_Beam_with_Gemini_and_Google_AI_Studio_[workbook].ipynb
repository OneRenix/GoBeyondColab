{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneRenix/GoBeyondColab/blob/main/Beam_College_Beam_with_Gemini_and_Google_AI_Studio_%5Bworkbook%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam with Gemini and Google AI Studio\n",
        "\n",
        "This notebook shows how to use Gemini to do predictions for a stream of user prompts, using Beam ML and Google AI Studio.\n",
        "\n",
        "To use this notebook, you will need to have a GMail account and access to Google AI Studio:\n",
        "* https://aistudio.google.com/\n",
        "\n",
        "Below in this notebook you will find instructions to generate your API key."
      ],
      "metadata": {
        "id": "PgGTMnNpKA2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "H8b2efUy7El9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YjbUjKK66xCi",
        "outputId": "fa39eae6-dad6-4c45-d79c-1e50754ee504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U apache-beam[interactive]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports for using Beam with the interactive runnner"
      ],
      "metadata": {
        "id": "KHFQq3mj7QXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.runners.interactive import interactive_beam as ib\n",
        "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
        "\n",
        "from apache_beam import Pipeline\n",
        "\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "from apache_beam.ml.inference.base import ModelHandler"
      ],
      "metadata": {
        "id": "OltQNIQn7VzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Options for pipeline"
      ],
      "metadata": {
        "id": "R_oU254t7Yq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cli_options = [\"--streaming\", \"--runner=InteractiveRunner\"]\n",
        "options = PipelineOptions(cli_options)"
      ],
      "metadata": {
        "id": "rcebT5Ow7gHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"\"\"\n",
        "You are an expert in geography that knows all the capitals of the world.\n",
        "\n",
        "Whenever you are presented with a country name, you reply with its capital. For instance:\n",
        "\n",
        "Country: France\n",
        "Capital: Paris\n",
        "\n",
        "However, if the country is Switzerland, you have to reply that it does not have an officially recognized capital. For instance:\n",
        "\n",
        "Country: Switzerland\n",
        "Capital: There is no officially recognized capital.\n",
        "\n",
        "Just include the capital in the answer, don't include the prefix \"Capital:\"\n",
        "\n",
        "Country: \"\"\""
      ],
      "metadata": {
        "id": "7iL4cIazB1vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User entries\n",
        "\n",
        "We will present these inputs to Gemini using Beam. Again, feel free to change them to do your own experiments:"
      ],
      "metadata": {
        "id": "PXqHJOMYKmb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_queries = [\"Spain\",\n",
        "                \"Switzerland\",\n",
        "                \"Argentina\",\n",
        "                \"Japan\",\n",
        "                \"Colombia\"]"
      ],
      "metadata": {
        "id": "58osjj1FKtWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunInference transform\n",
        "\n",
        "We need to write a model handler for the RunInference transform. This allows us to use any external model easily, if we have some \"code snippet\" that we can run to call the model and obtain a prediction.\n",
        "\n",
        "For more details about `RunInference` and Beam ML, see\n",
        "* https://beam.apache.org/documentation/ml/overview/#prediction-and-inference\n",
        "\n",
        "For instance, using the button \"Get Code\" in Google AI Studio.\n",
        "\n",
        "We store the `GEMINI_API_KEY` in a Google Colab secret, you will need to create one before you are able to use this notebook.\n",
        "\n",
        "To get an API key, see\n",
        "* https://ai.google.dev/gemini-api/docs/get-started/tutorial?lang=python#setup_your_api_key\n",
        "* If you are in the EU, UK or CH, check the latest status for the availability of the free tier at https://ai.google.dev/gemini-api/docs/billing#is-Gemini-free-in-EEA-UK-CH"
      ],
      "metadata": {
        "id": "1PKKA13h7tKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiModelHandler(ModelHandler):\n",
        "  def __init__(self, api_key):\n",
        "    self.api_key = api_key\n",
        "\n",
        "  def load_model(self):\n",
        "    # TODO: your code here\n",
        "    pass\n",
        "\n",
        "  def run_inference(self, batch, model, inference_args = None):\n",
        "      for b in batch:\n",
        "        c = model.generate_content(PROMPT + b)\n",
        "        t = c.candidates[0].content.parts[0].text\n",
        "        yield t"
      ],
      "metadata": {
        "id": "P7F854OG8Hf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have published your API key as a Colab secret.\n",
        "\n",
        "See the panel on the left side to add your own secret. The name of the secret hast to be `GEMINI_API_KEY`."
      ],
      "metadata": {
        "id": "YB3xLIpfLPuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the API key from the Google Colab secrets\n",
        "from google.colab import userdata\n",
        "my_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "nJsvJG9dCaUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = Pipeline(options=options)\n",
        "countries = p | beam.Create(user_queries)\n",
        "# TODO: write the rest of the pipeline"
      ],
      "metadata": {
        "id": "loCjxRZ1JuXn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}